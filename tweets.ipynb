{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LDfWLvhYPtO"
   },
   "source": [
    "# Classifying Tweets: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PmFK0GgYPtU"
   },
   "source": [
    "## Problem \n",
    "\n",
    "In this problem, Twitter data is analyzed and extracted using [this](https://dev.twitter.com/overview/api) api. The data contains tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "For every tweet, there are two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "The tweets have been divided into two parts - train and test available in CSV files. For train, both the `screen_name` and `text` attributes are provided but for test, `screen_name` is hidden.\n",
    "\n",
    "The goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (true class labels) is determined from the `screen_name` of the tweet as follows\n",
    "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
    "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
    "\n",
    "This is a binary classification problem. \n",
    "\n",
    "The problem proceeds in three stages:\n",
    "- **Text processing**: clean up the raw tweet text using the various functions offered by the [nltk](http://www.nltk.org/genindex.html) package.\n",
    "- **Feature construction**: construct bag-of-words feature vectors and training labels from the processed text of tweets and the `screen_name` columns respectively.\n",
    "- **Classification**: use [sklearn](http://scikit-learn.org/stable/modules/classes.html) package to learn a model which classifies the tweets as desired. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vogrCA5aYPtQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn \n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Latex, Markdown\n",
    "import copy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` and `sklearn` are used in this problem. NLTK comes with many toy grammars, trained models, etc, which have to be downloaded manually. This assignment requires NLTK's stopwords list and WordNetLemmatizer. Install them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQGjuhliYPtV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "\n",
    "# Verify that the following commands work for you, before moving on.\n",
    "\n",
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTqEaBw9YPtZ"
   },
   "source": [
    "## A. Text Processing\n",
    "\n",
    "A function was created to process and tokenize raw text. The generated list of tokens meet the following specifications:\n",
    "  1. The tokens are all in lower case.\n",
    "  2. The tokens appear in the same order as in the raw text.\n",
    "  3. The tokens are in their lemmatized form. If a word cannot be lemmatized (i.e, an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
    "  4. The tokens do not contain any punctuations. Punctuations are handled as follows: \n",
    "    1. Apostrophe of the form `'s` are ignored. e.g., `She's` becomes `she`. \n",
    "    2. Other apostrophes are omitted. e.g, `don't` becomes `dont`. \n",
    "    3. Words are broken at the hyphen and other punctuations. \n",
    "  5. The tokens do not contain any part of a url.\n",
    "\n",
    "`string.punctuation` is used to get hold of all punctuation symbols. \n",
    "[Regular expressions](https://docs.python.org/3/library/re.html) are used for capturing urls in the text. \n",
    "\n",
    "Tokens must be of type `str`. `nltk.word_tokenize()` is used for tokenization once punctuation is handled. \n",
    "\n",
    "`lemmatize()` function [here](https://www.nltk.org/_modules/nltk/stem/wordnet.html).\n",
    "\n",
    "In order for `lemmatize()` to give the root form for any word, the context in which to lemmatize through the `pos` parameter (`lemmatizer.lemmatize(word, pos=SOMEVALUE)`) has to be provided. The context is the part of speech (POS) for that word. [nltk.pos_tag()](https://www.nltk.org/book/ch05.html) gives the lexical categories for each word. The results from `pos_tag()` are then used for the `pos` parameter.\n",
    "\n",
    "However, the POS tag returned from `pos_tag()` is in different format than the expected pos by `lemmatizer`.\n",
    "> pos\n",
    "(Syntactic category): n for noun files, v for verb files, a for adjective files, r for adverb files.\n",
    "\n",
    "These pos need to be mapped appropriately. `nltk.help.upenn_tagset()` provides description of each tag returned by `pos_tag()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5IMtsQoYPta"
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "  \"\"\" Normalizes case and handles punctuation\n",
    "  Inputs:\n",
    "  text: str: raw text\n",
    "  lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "              (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "  Outputs:\n",
    "  list(str): tokenized text\n",
    "  \"\"\"\n",
    "\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) # remove url\n",
    "  text = text.replace(\"'s\", '') # remove possessive 's \n",
    "  text = text.replace(\"'\", '') # remove other apostrophes\n",
    "  text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) # remove punctuation\n",
    "  text = re.sub(' +', ' ', text) # remove whitespace b/w words\n",
    "  text = text.strip() # remove leading or trailing whitespace\n",
    "\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "  pos_mapping = {\n",
    "    \"N\":'n',\n",
    "    \"V\":'v',\n",
    "    \"J\":'a',\n",
    "    \"R\":'r'\n",
    "  }\n",
    "\n",
    "  lemmatized = []\n",
    "  for word, pos in tokens:\n",
    "    tag = pos_mapping.get(pos[0])\n",
    "    if (tag != None):\n",
    "      lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "    else:\n",
    "      lemmatized.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "  return lemmatized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bspITcPiYPte"
   },
   "source": [
    "Test the above function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrEzaolBYPtf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'do', 'well', 'how', 'about', 'you']\n",
      "['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
      "['be', 'have', 'do', 'language', 'city', 'mice']\n",
      "['it', 'hilarious', 'check', 'it', 'out']\n"
     ]
    }
   ],
   "source": [
    "print(process(\"I'm doing well! How about you?\"))\n",
    "# ['im', 'do', 'well', 'how', 'about', 'you']\n",
    "\n",
    "print(process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"))\n",
    "# ['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
    "\n",
    "print(process(\"been had done languages cities mice\"))\n",
    "# ['be', 'have', 'do', 'language', 'city', 'mice']\n",
    "\n",
    "print(process(\"It's hilarious. Check it out http://t.co/dummyurl\"))\n",
    "# ['it', 'hilarious', 'check', 'it', 'out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylcws_-LYPti"
   },
   "source": [
    "Now use the `process()` function implemented to convert the pandas dataframe just loaded from tweets_train.csv file. The function is able to handle any data frame which contains a column called `text`. The data frame returned replaces every string in `text` with the result of `process()` and retains all other columns as such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEONT9fXYPti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
      "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
      "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
      "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
      "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ...\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM7hJBdOYPtl"
   },
   "outputs": [],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "  \"\"\" process all text in the dataframe using process() function.\n",
    "  Inputs\n",
    "    df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "    lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "      (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "  Outputs\n",
    "    pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "      the output from process() function. Other columns are unaffected.\n",
    "  \"\"\"\n",
    "  df['text'] = df['text'].apply(process)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXQP6CAiYPto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
      "1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
      "2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
      "3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
      "4        timkaine  [glad, the, senate, could, pass, a, thud, milc...\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "processed_tweets = process_all(tweets)\n",
    "print(processed_tweets.head())\n",
    "\n",
    "#       screen_name                                               text\n",
    "# 0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
    "# 1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
    "# 2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
    "# 3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
    "# 4        timkaine  [glad, the, senate, could, pass, a, thud, milc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsBiunCIYPtr"
   },
   "source": [
    "## B. Feature Construction\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, a bag-of-words TF-IDF feature vector is construted. But before that, the number of possible words is large and not all of them may be useful for the classification task. I determined which words to retain, and which to omit. \n",
    "> \"A common heuristic is to construct a frequency distribution of words in the corpus and prune out the head and tail of the distribution. The intuition of the above operation is as follows.\" Very common words (i.e. stopwords) add almost no information regarding similarity of two pieces of text. Similarly with very rare words. NLTK has a list of in-built stop words which is a good substitute for head of the distribution. A word is considered rare if it occurs only in a single document (row) in whole of `tweets_train.csv`. \n",
    "\n",
    "A sparse matrix of features is constructed for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). `min_df=2` is passed to filter out the words occuring only in one document in the whole training set. Stop words are ignored. Other optional parameters are left at their default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NQ9wVm3YPt0"
   },
   "outputs": [],
   "source": [
    "def create_features(processed_tweets, stop_words):\n",
    "  \"\"\" creates the feature matrix using the processed tweet text\n",
    "  Inputs:\n",
    "    tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'\n",
    "    stop_words: list(str): stop_words by nltk stopwords\n",
    "  Outputs:\n",
    "    sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "      we need this to tranform test tweets in the same way as train tweets\n",
    "    scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "  \"\"\"\n",
    "  docs2 = copy.deepcopy(processed_tweets['text'])\n",
    "  docs = [' '.join(i) for i in docs2]\n",
    "\n",
    "\n",
    "  vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(use_idf = True, min_df = 2, stop_words = stop_words)\n",
    "  vectors = vectorizer.fit_transform(docs)\n",
    "\n",
    "  return vectorizer, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYtqEyvYPt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17298, 7923)\n"
     ]
    }
   ],
   "source": [
    "# execute this code \n",
    "(tfidf, X) = create_features(processed_tweets, stopwords)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WQJih7SYPt6"
   },
   "source": [
    "For each tweet, assign a class label (0 or 1) using its `screen_name`. Use 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLtDPaInYPt7"
   },
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "  \"\"\" creates the class labels from screen_name\n",
    "  Inputs:\n",
    "    tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "  Outputs:\n",
    "    numpy.ndarray(int): dense binary numpy array of class labels\n",
    "  \"\"\"\n",
    "  labels = []\n",
    "  initialize = 1\n",
    "  for user in processed_tweets['screen_name']:\n",
    "    if (initialize == 1):\n",
    "      if (user == 'realDonaldTrump' or user == 'mike_pence' or user == 'GOP'):\n",
    "        labels = np.array(0, dtype = int)\n",
    "      else:\n",
    "        labels = np.array(1, dtype = int)\n",
    "      initialize = 0\n",
    "    else:\n",
    "      if(user == 'realDonaldTrump' or user == 'mike_pence' or user == 'GOP'):\n",
    "        labels = np.append(labels, 0)\n",
    "      else:\n",
    "        labels = np.append(labels, 1)\n",
    "\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z49a4djKYPt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17298\n"
     ]
    }
   ],
   "source": [
    "# execute this code\n",
    "y = create_labels(processed_tweets)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5STbtJYSYPuA"
   },
   "source": [
    "## C. Classification\n",
    "\n",
    "The classifier used is [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (Support Vector Machine). \n",
    "\n",
    "At the heart of SVMs is the concept of kernel functions, which determines how the similarity/distance between two data points is computed. `sklearn`'s SVM provides four kernel functions: `linear`, `poly`, `rbf`, `sigmoid` (details [here](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)) (**IDEA: implement own distance function and pass it as an argument to the classifier**).\n",
    "\n",
    "Tasks for classifier:\n",
    "\n",
    "1. Implement and evaluate a simple baseline classifier MajorityLabelClassifier.\n",
    "2. Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. \n",
    "3. Implement the `evaluate_classifier()` function which scores a classifier based on accuracy of a given dataset.\n",
    "4. Implement `best_model_selection()` to perform cross-validation by calling `learn_classifier()` and `evaluate_classifier()` for different folds and determine which of the four kernels performs the best.\n",
    "5. Go back to `learn_classifier()` and fill in the best kernel. \n",
    "\n",
    "\n",
    "To determine whether the classifier is performing well, compare it to a baseline classifier. A baseline is generally a simple or trivial classifier and the classifier implemented should beat the baseline in terms of a performance measure such as accuracy. The implemented classifier, `MajorityLabelClassifier`, always predicts the class equal to the **mode** of the labels (i.e., the most frequent label) in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUBpZ6_NYPuB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4998265695456122\n"
     ]
    }
   ],
   "source": [
    "class MajorityLabelClassifier():\n",
    "  \"\"\"\n",
    "  A classifier that predicts the mode of training labels\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Initialize\n",
    "    \"\"\"\n",
    "    self.mode = None\n",
    "\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    Implement fit by taking training data X and their labels y and finding the mode of y\n",
    "    \"\"\"\n",
    "\n",
    "    m = stats.mode(y)\n",
    "    self.mode = int(m[0])\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Implement to give the mode of training labels as a prediction for each data instance in X\n",
    "    return labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels = []\n",
    "    for i in X:\n",
    "      labels.append(self.mode)\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Report the accuracy of your classifier by comparing the predicted label of each example to its true label\n",
    "obj = MajorityLabelClassifier()\n",
    "obj.fit(X,y)\n",
    "preds = obj.predict(X)\n",
    "\n",
    "length = len(preds)\n",
    "i = 0\n",
    "count = 0\n",
    "while (i < length):\n",
    "  if (preds[i] != y[i]):\n",
    "      count = count + 1\n",
    "  i = i + 1\n",
    "pred_acc = count / length\n",
    "print(pred_acc)\n",
    "\n",
    "# training accuracy = 0.500173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZldwD1JYPuD"
   },
   "source": [
    "Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. Default values are used for any other optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGpfOK2EYPuD"
   },
   "outputs": [],
   "source": [
    "def learn_classifier(X_train, y_train, kernel):\n",
    "  \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "  Inputs:\n",
    "    X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "    y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "    kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "  Outputs:\n",
    "    sklearn.svm.classes.SVC: classifier learnt from data\n",
    "  \"\"\"\n",
    "\n",
    "  classifier = sklearn.svm.SVC(kernel = kernel)\n",
    "  classifier.fit(X_train, y_train)\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOwIKVuvYPuF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# execute code\n",
    "classifier = learn_classifier(X, y, 'linear')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDsVwZuNYPuK"
   },
   "source": [
    "The next step is to evaluate the classifier, ie., characterize how good its classification performance is. (This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model).\n",
    "\n",
    "There are two questions that now come to mind:\n",
    "1. **What data to use?** \n",
    "    - **Validation Data**: The data used to evaluate a classifier is called **validation data** (or hold-out data), and it is usually different from the data used for training. The model or hyperparameter with the best performance in the held out data is chosen. This approach is relatively fast and simple but vulnerable to biases found in validation set.\n",
    "    - **Cross-validation**: This approach divides the dataset in $k$ groups (so, called k-fold cross-validation). One of group is used as test set for evaluation and other groups as training set. The model or hyperparameter with the best average performance across all k folds is chosen. For this question you will perform 4-fold cross validation to determine the best kernel. We will keep all other hyperparameters default for now. This approach provides robustness toward biasness in validation set. However, it takes more time.\n",
    "    \n",
    "\n",
    "2. **And what metric?** \n",
    "  - There are several evaluation measures available in the literature (e.g., accuracy, precision, recall, F-1,etc) and different fields have different preferences for specific metrics due to different goals. We will go with accuracy. According to wiki, **accuracy** of a classifier measures the fraction of all data points that are correctly classified by it; it is the ratio of the number of correct classifications to the total number of (correct and incorrect) classifications. `sklearn.metrics` provides a number of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvLsjxqKYPuL"
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "  \"\"\" evaluates a classifier based on a supplied validation data\n",
    "  Inputs:\n",
    "    classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "    X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "    y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "  Outputs:\n",
    "    double: accuracy of classifier on the validation data\n",
    "  \"\"\"\n",
    "\n",
    "  predictions = classifier.predict(X_validation)\n",
    "  accuracy = accuracy_score(y_validation, predictions) \n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmdMkFIzYPuN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951959764134582\n"
     ]
    }
   ],
   "source": [
    "# test code by evaluating the accuracy on the training data\n",
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy) \n",
    "# should give 0.956700196554515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYYIADjZYPuP"
   },
   "source": [
    "Now it is time to decide which kernel works best by using the cross-validation technique. The training data is split into 4-folds (75% training and 25% validation) by shuffling randomly. For each kernel, the average accuracy for all folds is recorded and the best classifier is determined. Since the dataset is balanced (both classes are in almost equal propertion), `sklearn.model_selection.KFold` [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) can be used for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq5AlMoUYPuQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=4, random_state=1, shuffle=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Og6hzUPPYPuT"
   },
   "source": [
    "Then determine which classifier is the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJAjQyNKYPuT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly\n"
     ]
    }
   ],
   "source": [
    "def best_model_selection(kf, X, y):\n",
    "  \"\"\"\n",
    "  Select the kernel giving best results using k-fold cross-validation.\n",
    "  Other parameters should be left default.\n",
    "  Input:\n",
    "  kf (sklearn.model_selection.KFold): kf object defined above\n",
    "  X (scipy.sparse.csr.csr_matrix): training data\n",
    "  y (array(int)): training labels\n",
    "  Return:\n",
    "  best_kernel (string)\n",
    "  \"\"\"\n",
    "  best_kernel = 'linear'\n",
    "  best_accuracy = 0\n",
    "\n",
    "  for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "    accuracy = []\n",
    "    for train, test in kf.split(X):\n",
    "      # Split train-test\n",
    "      X_train, X_test = X[train], X[test]\n",
    "      y_train, y_test = y[train], y[test]\n",
    "      classifier = learn_classifier(X_train, y_train, kernel)\n",
    "      accuracy.append(evaluate_classifier(classifier, X_test, y_test))\n",
    "\n",
    "    if (sum(accuracy) / 4 > best_accuracy):\n",
    "      best_accuracy = sum(accuracy) / 4\n",
    "      best_kernel = kernel\n",
    "\n",
    "  return best_kernel\n",
    "\n",
    "\n",
    "#Test your code\n",
    "best_kernel = best_model_selection(kf, X, y)\n",
    "print(best_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yho_UYX7YPuX"
   },
   "source": [
    "A wrapper function that uses the model to classify unlabeled tweets from tweets_test.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qth7DPekYPuY"
   },
   "outputs": [],
   "source": [
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "  \"\"\" predicts class labels for raw tweet text\n",
    "  Inputs:\n",
    "    tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "    classifier: sklearn.svm.classes.SVC: classifier learnt\n",
    "    unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "  Outputs:\n",
    "    numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "  \"\"\"\n",
    "  tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "  train_tweets = process_all(tweets)\n",
    "  tfidf, X = create_features(train_tweets, stopwords)\n",
    "  y = create_labels(train_tweets)\n",
    "\n",
    "  classifier = learn_classifier(X, y, 'poly')\n",
    "\n",
    "  tfidf, X_test = create_features(test_tweets, stopwords)\n",
    "  classifier.predict(X_test)\n",
    "\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKNMwVVgYPua"
   },
   "outputs": [],
   "source": [
    "# **TO-DO** Fill in best classifier in the function and re-trian classifier using all training data\n",
    "# **TO-DO** Get predictions for unlabelled test data\n",
    "#classifier = learn_classifier(X, y, best_kernel)\n",
    "#unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "#y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeuZhVR2OhyV"
   },
   "source": [
    "## Closing Questions\n",
    "\n",
    "Did the SVM classifier perform better than the baseline?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
